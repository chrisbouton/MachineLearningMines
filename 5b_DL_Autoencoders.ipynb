{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 470 Activities and Case Studies\n",
    "\n",
    "1. For all activities, you are allowed to collaborate with a partner. \n",
    "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
    "\n",
    "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations with regard to how these notebooks will be graded:\n",
    "\n",
    "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
    "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
    "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
    "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
    "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
    "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
    "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
    "7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
    "8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5bc91cffca4c7fcddc138415994de5d",
     "grade": false,
     "grade_id": "cell-02f2e430dc0f2177",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Deep Learning - Autoencoders\n",
    "\n",
    "In this exercise we'll use an AutoEncoder to learn a dimenionally reduced representation of data and investigate its performance compared to using the original data. You'll learn how to build AutoEncoders and how to use the Keras functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bfa63f9cefeb60ef",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model, Input\n",
    "import sklearn as sk\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef6a42882991834b",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "features = data[\"data\"]\n",
    "targets = data[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read through the description of the data to better understand it\n",
    "# What features do we have and what is the target we're trying to predict?\n",
    "print(data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00fdca3096d5954a",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d500a79191103d38",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee63a46921cf0ff7",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a91b741d4ee54822",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1dc4a56eec7732bb2329786a6e3f1f5",
     "grade": false,
     "grade_id": "cell-1176b78d06c61ed0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Building a Keras Model with the functional API\n",
    "\n",
    "In this exercise, instead of using the `Sequential` model, we will use the base `Model` in `tf.keras`. There are two approaches to using `tf.keras.Model`. We will use the functional API as outlined in the [`Model` docs](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model).\n",
    "\n",
    "Note that unlike the manner in which we defined our model in the prior Activity, in this Activity (using the base Model) the definition is more like that of the coding of a functional algorithm, e.g.:\n",
    "\n",
    "b = f1(a)  \n",
    "c = f2(b)  \n",
    "d = f3(c)  \n",
    "etc.\n",
    "\n",
    "Unlike traditional script execution in Python, however, those lines of code do not actually execute the computation at that moment. Rather, they are defining a chain of operations that our `Model` will execute later, when we ask it to.\n",
    "\n",
    "### The architecture of your autoencoder\n",
    "\n",
    "Below, you'll build an autoencoder with several layers. Recall that an encoder is composed of an \"encoder\" portion and a \"decoder\" portion. Your encoder will have two layers, transforming the number of features down to 10 in the first layer, and down to 5 (or less) in the second layer. The output of that second layer will serve as the encoded (or \"embedded\") representation, which will later be used as features for an SVM model. Your decoder will also have two layers, undoing the encoding, and transforming the encoded representation up to 10 in the first layer, and up to the original dimensionality in its second layer.\n",
    "\n",
    "Your autoencoder model with thus have 4 layers of neurons. Some would call this a 5-layer model, considering the input samples as a \"layer\" as well, although that is not a layer of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53b86a7e80ac5f197c5a3e7ae190ec5d",
     "grade": false,
     "grade_id": "cell-b228946b2927aed3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Determine the number of input dimensions (features) and use that value to\n",
    "# create a tf.keras.Input object, giving it the variable name \"inputs\".\n",
    "#\n",
    "# Also, select a dimension (<=5) for the encoding/embedding (the number of\n",
    "# neurons in the \"middle\" layer of your autoencoder) and give it the\n",
    "# variable name \"embedding_dim\".\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b7026c4683712085ae0617034084db2",
     "grade": true,
     "grade_id": "cell-3cc87f89c7f8bf0f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert inputs.shape[1] == X_train.shape[1]\n",
    "assert isinstance(embedding_dim, int)\n",
    "assert embedding_dim > 0 and embedding_dim <= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ad2ce2a2cef986ce7a88bb4dc84991b",
     "grade": false,
     "grade_id": "cell-18891b82369b7f2b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# To start, you'll define the encoding portion of the autoencoder.\n",
    "#\n",
    "# Start with \"inputs\" and chain layer calls to two subsequent Dense layers,\n",
    "# the first with 10 neurons (units), the second with \"embedding_dim\" neurons.\n",
    "# See the tf.keras.Model documentation (linked in the instructional cell\n",
    "# above). There is a brief example near the top of that webpage.\n",
    "#\n",
    "# Use ReLU as the activation function for the first Dense layer\n",
    "# and do not set an activation for the second Dense layer.\n",
    "#\n",
    "# Name the output of the second Dense layer \"encoded\".\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31abc5cfc4f7c5a5576572024e68b292",
     "grade": true,
     "grade_id": "cell-4bec706b02502c9c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "testM = Model(inputs, encoded)\n",
    "assert len(testM.layers) == 3\n",
    "assert encoded.shape[1] == embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "706a2b69ebbb5ef265395058ba85af34",
     "grade": false,
     "grade_id": "cell-4331d11179de97f3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Now you'll define the decoding portion of the autoencoder.\n",
    "#\n",
    "# Chain layer calls to two more dense layers, the first with 10 neurons and the\n",
    "# second (final layer) with the same number of neurons as your input (number\n",
    "# of features).\n",
    "#\n",
    "# Use ReLU as the activation function for the first new Dense layer\n",
    "# and do not set an activation for the second new Dense layer.\n",
    "#\n",
    "# Name the output of the final Dense layer \"decoded\".\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "226c9a566578efa0ca1455d7f48e6c19",
     "grade": true,
     "grade_id": "cell-3ea6527c01beb144",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "testM = Model(inputs, decoded)\n",
    "print(len(testM.layers))\n",
    "assert len(testM.layers) == 5\n",
    "assert decoded.shape[1] == 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the autoencoder\n",
    "\n",
    "Above, you defined the encoder, decoder, and collectively the autoencoder. But we haven't actually instantiated any anything yet.\n",
    "\n",
    "In the cell below, we'll create/instantiate your autoencoder for you, and also create a separate \"encoder\" object which shares its layers with the encoder portion of the autoencoder. This makes it easy for use to train the full autoencoder, and then use just the encoder portion to convert our original features into an embedded representation of lower dimensionality. We'll also create a separate \"decoder\" object in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2205a928bea57abe",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the autoencoder\n",
    "autoencoder = Model(inputs, decoded)\n",
    "\n",
    "\n",
    "# Create the encoder, which takes the same inputs as the\n",
    "# autoencoder, but stops after the encoding layers. Thus,\n",
    "# the output of the encoder is the encoded representation.\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "\n",
    "# Create the decoder which starts at the encoded output,\n",
    "# and uses the remaining layers of the autoencoder...\n",
    "encoded_embedding = Input(shape=(embedding_dim,))\n",
    "\n",
    "# Get the 1st and 2nd decoder layer from the autoencoder\n",
    "decoder_layer2 = autoencoder.layers[-2]\n",
    "decoder_layer3 = autoencoder.layers[-1]\n",
    "\n",
    "# Chain layer calls\n",
    "decoder_out = decoder_layer3(decoder_layer2(encoded_embedding))\n",
    "\n",
    "# Create the decoder\n",
    "decoder = Model(encoded_embedding, decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25699d121c563596",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# View the autoencoder architecture\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the encoder architecture\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the decoder architecture\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Below we'll compile and train the model. __Notice that in our call to `fit` we use `X_train` as both the features and the targets__. Our autoencoder is not a traditional machine learning model. It uses self-supervised learning, in which we want the output to equal the input. This might seem easy, but we are forcing the autoencoder model to compress the features down to a much lower dimensionality, in the bottlenecked autoencoder architecture. Thus, it may have to learn a complex non-linear function to accomplish this task.\n",
    "\n",
    "Note that, as always, we do not use the test set features to train the autoencoder. Test set features are held out for all stages of training including dimensionality reduction.\n",
    "\n",
    "We'll train for quite a while (1000 epochs). If all goes well, you'll see afterwards as to why we train for such a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-92f803b5ba04dadd",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model, using the Adam optimizer for gradient descent,\n",
    "# and using MSE as the loss function.\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Train our model. Note that X_train serves as both features and targets.\n",
    "n_epochs = 1000\n",
    "history = autoencoder.fit(X_train, X_train, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our the loss scores collected during the training/fit session above.\n",
    "\n",
    "plt.semilogy(np.arange(1, n_epochs+1), history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss during training session.')\n",
    "\n",
    "print(f\"Loss on the final training epoch was {history.history['loss'][-1]:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training convergence\n",
    "\n",
    "If everthing went as planned, you'll see a somewhat chaotic, stairstep-shapped loss curve in the figure above. During training, the model parameters sometimes got stuck in or a near a local minimum of the loss function, which is why you see some flatter portions even during the early training epochs. Fortunately the learning procedure found a way out of those local minima.\n",
    "\n",
    "### Embedding\n",
    "\n",
    "Now that you've trained the autoencoder, and given yourself direct access to the encoder half of the autoencoder, we'll use that encoder (below) to \"embed\" the original features into a new feature/embedding space. The resulting new features are typically called \"embedded\" or \"encoded\" features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf4fdda42d853412",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the embedded features using the encoder model\n",
    "X_train_embed = encoder.predict(X_train)\n",
    "X_test_embed = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "Now comes the actual training of a supervised learning model. We have our embedded features, thanks to our autoencoder, along with our original features.\n",
    "\n",
    "__You'll train two SVM models to predict breast cancer diagnoses: malignant or benign.__  \n",
    "One SVM will use the original features and one will use the embedded features.  \n",
    "Which do you think well make better test set predictions?\n",
    "\n",
    "You may experience some `ConvergenceWarning` messages. That's okay, you can ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56c7035cf4e5032e5bbbb1ce9d7d4d35",
     "grade": false,
     "grade_id": "cell-740d35aaf5aced68",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Train two LinearSVC models\n",
    "#\n",
    "# Create and fit a model with the original features.\n",
    "# Name the model \"base_model\".\n",
    "#\n",
    "# Create and fit a model with autoencoder-embedded features.\n",
    "# Name the model \"embed_model\".\n",
    "#\n",
    "# When you create your LinearSVC models, you may want to\n",
    "# set random_seed to the same values (e.g., 0) for both models\n",
    "# for a more apples-to-apples comparison.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00d0db0002d2e5bce8165b07daf5879a",
     "grade": true,
     "grade_id": "cell-fcb552f3048b39de",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert base_model\n",
    "assert isinstance(base_model, LinearSVC)\n",
    "assert base_model.coef_.shape[1] == 30\n",
    "assert embed_model\n",
    "assert isinstance(embed_model, LinearSVC)\n",
    "assert embed_model.coef_.shape[1] == embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a4569d596e6d9513",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The base SVM accuracy score:                  {base_model.score(X_test, y_test):0.3f}\")\n",
    "print(f\"The autoencoder embedding SVM accuracy score: {embed_model.score(X_test_embed, y_test):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parting thoughts\n",
    "\n",
    "Was the test set score of the embedded data model better or worse than that of the original data model?\n",
    "\n",
    "Ask youself why it might be better or worse. \n",
    "\n",
    " - What happens when you change the activation function(s) in the autoencoder?\n",
    " - What happens when you change the embedding_dim to be larger or smaller?\n",
    " - Is it sufficient to just use LinearSVC with the default parameters to make any of these conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f548879bd197ac235bc863b352b6483",
     "grade": false,
     "grade_id": "cell-1095f38e46bb02b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
     "grade": false,
     "grade_id": "feedback",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feedback():\n",
    "    \"\"\"Provide feedback on the contents of this exercise\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
     "grade": true,
     "grade_id": "feedback-tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
